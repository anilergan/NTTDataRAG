# app.py
# poetry run uvicorn app:app --reload

import os
import json
import random
from typing import List
from fastapi import FastAPI, Request
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import OpenAI
import numpy as np
import faiss

# === YÃ¼klemeler ve ayarlar ===
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_api_key)

CHUNKS_PATH = "src/data/merged_chunks.jsonl"
INDEX_PATH = "src/data/faiss_index.faiss"

# === FastAPI nesnesi ===
app = FastAPI(title="NTT RAG Pipeline API")

# === Chunk ve Index yÃ¼kle ===
with open(CHUNKS_PATH, "r", encoding="utf-8") as f:
    chunks = [json.loads(line) for line in f]

index = faiss.read_index(INDEX_PATH)

# === Request-Response modelleri ===
class AskRequest(BaseModel):
    question: str

class AskResponse(BaseModel):
    answer: str
    sources: List[str]

# === SaÄŸlÄ±k kontrolÃ¼ ===
@app.get("/health")
def health_check():
    return {"status": "ok"}

# === Ana soru-cevap endpointâ€™i ===
@app.post("/ask", response_model=AskResponse)
def ask_question(request: AskRequest):
    question = request.question.strip()

    # ðŸ”¹ Embed soruyu
    resp = client.embeddings.create(
        model="text-embedding-3-small",
        input=question,
    )
    qvec = np.array(resp.data[0].embedding, dtype="float32").reshape(1, -1)

    # ðŸ”¹ FAISS aramasÄ±
    distances, indices_ = index.search(qvec, k=5)
    max_sim = distances[0][0]

    # ðŸ”¹ DÃ¼ÅŸÃ¼k benzerlik fallback
    if max_sim < 0.5:
        user_lang = "tr" if any(ch in question for ch in "Ã¼ÄŸÅŸÄ±Ã§Ã¶") else "en"
        fallback = {
            "en": [
                "Sorry, I couldn't find any relevant information in the available documents.",
                "I'm not confident enough to answer that based on the provided sources.",
                "This question seems to be outside the scope of the documents I'm trained on.",
            ],
            "tr": [
                "ÃœzgÃ¼nÃ¼m, elimdeki belgelerde bu soruya dair gÃ¼venilir bir bilgi bulamadÄ±m.",
                "Bu soruya mevcut kaynaklara dayanarak saÄŸlÄ±klÄ± bir yanÄ±t veremem.",
                "Bu soru, elimdeki belgelerin kapsamÄ±nÄ±n dÄ±ÅŸÄ±nda gÃ¶rÃ¼nÃ¼yor.",
            ],
        }
        return AskResponse(answer=random.choice(fallback[user_lang]), sources=[])

    # ðŸ”¹ BaÄŸlam oluÅŸtur
    retrieved = [chunks[i] for i in indices_[0]]
    context = "\n\n".join(
        f"[{c['main_title_of_page']} > {c['main_subtitle_of_page']} > {c['header']}] (Page {c['page']})\n{c['content']}"
        for c in retrieved
    )

    # ðŸ”¹ LLM'e gÃ¶nder
    messages = [
        {"role": "system", "content": "You are a helpful assistant answering questions based on company reports."},
        {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}"},
    ]
    chat = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.3,
    )
    answer = chat.choices[0].message.content.strip()

    # ðŸ”¹ KaynaklarÄ± toparla
    sources = [
        f"{c['source']} | Page {c['page']} | {c['header']} | Similarity: {distances[0][i]:.2f}"
        for i, c in enumerate(retrieved)
    ]

    return AskResponse(answer=answer, sources=sources)
